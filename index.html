<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A video model conditioned on any motion.">
  <meta name="keywords" content="Video Generation, Motion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Motion Prompting: Controlling Video Generation with Motion Trajectories</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-3NGMQ4W03X"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-3NGMQ4W03X');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- TODO -->
  <!--<link rel="icon" href="./static/images/favicon.svg">-->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Motion Prompting: Controlling Video Generation with Motion Trajectories</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://dangeng.github.io/" target="_blank">Daniel Geng</a><sup>1,2</sup>&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=LQvi5XAAAAAJ" target="_blank">Charles Herrmann</a><sup>1,+</sup>&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://hurjunhwa.github.io/" target="_blank">Junhwa Hur</a><sup>1</sup>&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://people.csail.mit.edu/fcole/" target="_blank">Forrester Cole</a><sup>1</sup>&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/zhang-serena/" target="_blank">Serena Zhang</a><sup>1</sup>&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="http://tobiaspfaff.com/" target="_blank">Tobias Pfaff</a><sup>1</sup>&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Op4nexcAAAAJ&hl=en" target="_blank">Tatiana Lopez-Guevara</a><sup>1</sup>&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="http://www.carldoersch.com/" target="_blank">Carl Doersch</a><sup>1</sup>&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://scholar.google.co.uk/citations?user=0ncQNL8AAAAJ&hl=en" target="_blank">Yusuf Aytar</a><sup>1</sup>&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://people.csail.mit.edu/mrub/" target="_blank">Michael Rubinstein</a><sup>1</sup>&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://chensun.me/index.html" target="_blank">Chen Sun</a><sup>1,3</sup>&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://www.oliverwang.info/" target="_blank">Oliver Wang</a><sup>1</sup>&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://andrewowens.com/" target="_blank">Andrew Owens</a><sup>2</sup>&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://deqings.github.io/" target="_blank">Deqing Sun</a><sup>1</sup>&nbsp;&nbsp;
            </span>
          </div>

          <div class="is-size-5 publication-authors" style="padding-top: 0.5rem;">
            <span class="author-block"><sup>1</sup>Google DeepMind &nbsp;&nbsp; </span>
            <span class="author-block"><sup>2</sup>University of Michigan &nbsp;&nbsp; </span>
            <span class="author-block"><sup>3</sup>Brown University &nbsp;&nbsp;</span>
            <span class="author-block"><sup>+</sup>Project Lead</span>
          </div>

          <div class="is-size-5 publication-authors" style="padding-top: 0.5rem;">
            <span class="author-block">CVPR 2025</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2412.02700"
                   target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- arXiv -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2412.02700"
                   target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- alphaXiv -->
              <span class="link-block">
                <a href="https://www.alphaxiv.org/abs/2412.02700v1"
                   target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>alphaXiv</span>
                </a>
              </span>
              <!-- Bibtex -->
              <span class="link-block">
                <a href="#bibtex"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-book"></i>
                  </span>
                  <span>BibTeX</span>
                  </a>
              </span>
              <br>
              <!-- Interacting Gallery -->
              <span class="link-block">
                <a href="./gallery_interacting.html"
                   target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>"Interacting" Gallery</span>
                </a>
              </span>
              <!-- Camera Control Gallery -->
              <span class="link-block">
                <a href="./gallery_depth_dolly.html"
                   target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Camera Control Gallery</span>
                </a>
              </span>
              <!-- Motion Transfer Gallery -->
              <span class="link-block">
                <a href="./gallery_transfer_monkey.html"
                   target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Motion Transfer Gallery</span>
                </a>
              </span>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

      <!--
      <h2 class="subtitle has-text-centered tldr" style="margin-top: 1rem;">
        <b>tl;dr:</b> We train a video generation model to be conditioned on <i>any</i> motion, and then design <b><i>motion prompts</i></b> to elicit a range of capabilities
      </h2>
      -->

      <!-- Videos -->
      <div class="columns is-multiline is-variable is-4 is-vcentered is-centered" style="padding-top: 0;">
        <div class="column is-four-fifths">
          <div class="has-text-centered">

            <div class="columns is-centered has-text-centered">
              <div class="column is-full-width">
                <h2 class="title is-3">Step 1: Train a Track Conditioned Video Model</h2>
              </div>
            </div>

            <video poster="" autoplay muted loop playsinline>
              <source src="./static/videos/main/teaser/teaser_part1.mp4" type="video/mp4">
            </video>

            <!-- Weird for this to be here, but whatever -->
            <div class="columns is-centered has-text-centered">
              <div class="column is-full-width" style="margin-top: 3rem; margin-bottom: 1.5rem;">
                <h2 class="title is-3">Step 2: Prompt the model with <i>Motion Prompts</i></h2>
              </div>
            </div>

          </div>
        </div>
      </div>


      <!-- Videos -->
      <div class="columns is-multiline is-variable is-4">
        <div class="column is-half">
          <div class="has-text-centered">

            <div class="subtitle-with-button" style="margin-bottom: 0.5rem;">
              <h2 class="title is-4">Object Control</h2>
              <a href="index.html#interacting" class="button is-secondary subtitle-button">See More</a>
            </div>

            <video poster="" autoplay muted loop playsinline>
              <source src="./static/videos/main/teaser/parrot.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="column is-half">
          <div class="has-text-centered">

            <div class="subtitle-with-button" style="margin-bottom: 0.5rem;">
              <h2 class="title is-4">Emergent Physics</h2>
              <a href="index.html#interacting" class="button is-secondary subtitle-button">See More</a>
            </div>

            <video poster="" autoplay muted loop playsinline>
              <source src="./static/videos/main/teaser/sand.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="column is-half">
          <div class="has-text-centered">

            <div class="subtitle-with-button" style="margin-bottom: 0.5rem;">
              <h2 class="title is-4">Control with Geometry</h2>
              <a href="index.html#primitives" class="button is-secondary subtitle-button">See More</a>
            </div>

            <video poster="" autoplay muted loop playsinline>
              <source src="./static/videos/main/teaser/bear.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="column is-half">
          <div class="has-text-centered">

            <div class="subtitle-with-button" style="margin-bottom: 0.5rem;">
              <h2 class="title is-4">Camera Control</h2>
              <a href="index.html#depth" class="button is-secondary subtitle-button">See More</a>
            </div>

            <video poster="" autoplay muted loop playsinline>
              <source src="./static/videos/main/teaser/arc_lake.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="column is-half">
          <div class="has-text-centered">

            <div class="subtitle-with-button" style="margin-bottom: 0.5rem;">
              <h2 class="title is-4">Object + Camera Control</h2>
              <a href="index.html#compositions" class="button is-secondary subtitle-button">See More</a>
            </div>

            <video poster="" autoplay muted loop playsinline>
              <source src="./static/videos/main/teaser/composition_golden.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="column is-half">
          <div class="has-text-centered">

            <div class="subtitle-with-button" style="margin-bottom: 0.5rem;">
              <h2 class="title is-4">Motion Transfer</h2>
              <a href="index.html#transfer" class="button is-secondary subtitle-button">See More</a>
            </div>

            <video poster="" autoplay muted loop playsinline>
              <source src="./static/videos/main/teaser/monkey_trees.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>


      <!-- How to Read -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified bordered-rounded">
            <h4 class="title is-5" id="caveat">How to Read these Visualizations</h2>
            <p>
              We visualize tracks and first frame inputs on the left and the generated video on the right. 
              The tracks have trails to indicate their trajectory, and are colored to 
              help distinguish them.
              Some motion prompts are created by converting user mouse inputs, in which case we visualize the mouse motions and drags
              by placing a cursor where the mouse is, and a hand if the user is dragging.
              <u style="color: red;"><i><b style="color: red;">
                Please note that this does not indicate that the videos are generated real-time. 
                In fact, it takes about 12 minutes to sample each video.</b></i></u> 
              Also, because these videos are 
              generated in one pass, the motion is <u style="color: red;"><i><b style="color: red;">not causal</b></i></u> 
              (<a href="#causal">see here</a> for an example of this).
            </p>
            <p>
              All videos on this page are cherry-picked from four samples, except for those in the 
              <a href="#uncurated">"uncurated" section</a>, in which we show uncurated samples generated by our model.
            </p>
          </div>
        </div>
      </div>


      <!-- Title -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width" style="margin-top: 3rem;">
          <h2 class="title is-3">Overview</h2>
        </div>
      </div>

      <!-- Description -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified">
            <p>
              We first train a video generation model conditioned on <i>any</i> motion. 
              To do this, we condition our model on <i>point trajectories</i>
              <a id="ref19back" href="#ref19">[19]</a>
              <a id="ref20back" href="#ref20">[20]</a>
              <a id="ref21back" href="#ref21">[21]</a> — an incredibly flexible representation of motion.
              This allows us to encode the motion of just single points or thousands of points,
              the motion of specific objects or of a global scene, and even occlusions and temporally sparse motion.
            </p>
            <p>
              To actually use this model, we construct <b>motion prompts</b>.
              In much the same way we might prompt an LLM, we can use motion prompts
              to tease out different capabilities from our video model.
              Depending on how we do this,
              we can elicit a large range of behavior, such as <a href="#interacting">object control</a>, 
              <a href="#interacting">emergent physics</a>, <a href="#depth">camera control</a>, 
              <a href="#compositions">simultaneous object and camera control</a>, 
              <a href="#drag_edit">drag-based image editing</a>,
              <a href="#motionmag">motion magnification</a>, and
              <a href="#transfer">motion transfer</a>.
            </p>
          </div>
        </div>
      </div>

    </div>
  </div>
</section>


<!-- Interacting with Images -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Title -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="interacting">Interacting with Images</h2>
      </div>
    </div>

    <!-- Videos -->
    <div class="columns is-multiline is-variable is-4" style="padding-top: 1rem;">
      <div class="column is-full">
        <div class="has-text-centered rounded-border-video">
          <video poster="" autoplay muted loop playsinline>
            <source src="./static/videos/main/motion_prompting_viz_fade.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    
    <!-- Description -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified" style="padding: 1rem 0;">
          <p>
            We illustrate one simple way to construct a motion prompt above. 
            We can take user mouse motions and drags, and place 
            a grid of tracks wherever the mouse is being dragged.
            The result is similar to prior and concurrent work on sparse trajectory control
            <a id="ref1back" href="#ref1">[1]</a>
            <a id="ref2back" href="#ref2">[2]</a>
            <a id="ref3back" href="#ref3">[3]</a>
            <a id="ref4back" href="#ref4">[4]</a>
            <a id="ref5back" href="#ref5">[5]</a>
            <a id="ref6back" href="#ref6">[6]</a>
            <a id="ref7back" href="#ref7">[7]</a>
            <a id="ref8back" href="#ref8">[8]</a>
            <a id="ref9back" href="#ref9">[9]</a>
            <a id="ref10back" href="#ref10">[10]</a>
            <a id="ref11uack" href="#ref11">[11]</a>,
            but due to the flexibility of our motion representation, 
            we can also drag multiple times, release the mouse, or pin the background still with static tracks.
          </p>
          <p>
            We also find that some inputs result in emergent phenomena, 
            such as with the smoke, pine tree, sand, hair, or cow 
            below. This is particularly exciting
            because it shows the potential for video models to be general world models,
            and the potential for motion to be a way of interacting with and querying these world models.
            In addition, <u style="color: red;"><i><b style="color: red;">while the 
            results below are neither real-time nor causal</b></i></u> (<a href="#caveat">see above</a>)
            we believe that they show the potential of future video generation models 
            as they get faster, more efficient, and more powerful.
          </p>
        </div>
      </div>
    </div>

    <!-- Videos -->
    <div class="columns is-multiline is-variable is-4">

      <div class="column is-full">
        <div class="has-text-centered">
          <video class="lazy-video" poster="" muted loop playsinline>
            <source data-src="./static/videos/main/interact.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>

    <!-- Side note on prediction -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified bordered-rounded">
          <h4 class="title is-5" id="caveat">Track Conditioning enables Prediction</h2>
          <p>
            One interesting thing to note: Because our method works for temporally sparse track conditioning, we can effectively do prediction.
            For example, in the above samples we interact with an image briefly, roughly asking questions such as <i>"what happens if 
            I shake the branches of this tree?"</i> or <i>"how will the hair behave if I pull on it?"</i>
            In fact, in theory any temporally sparse conditioning signal should enable this capability.
            This suggests that scaling and improving these models may eventually lead to systems
            able to make predictions and answer counterfactuals about the world.
          </p>
        </div>
      </div>
    </div>

    <!-- Button to Gallery -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <a href="gallery_interacting.html" class="see-more-button button is-secondary" target="_blank">
          See more examples
        </a>
      </div>
    </div>

  </div>
</section>


<!-- Depth -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Title -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="depth">Camera Control from Depth</h2>
      </div>
    </div>

    <!-- Videos -->
    <!--
    <div class="columns is-multiline is-variable is-4" style="padding-top: 1rem;">
      <div class="column is-full">
        <div class="has-text-centered rounded-border-video">
          <video class="lazy-video" poster="" muted loop playsinline>
            <source data-src="./static/videos/main/depth_viz_fade.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    -->
    
    <!-- Description -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            Beyond single drags, we can also design motion prompts to achieve camera control.
            We do this by first running a monocular depth estimator 
            <a id="ref12back" href="#ref12">[12]</a> to get a point cloud, and then
            projecting points on to a user-provided sequence of cameras, defining the
            desired camera trajectory.
          </p>
          <p>
            By doing this, we can move the camera in <a href="#camera_arcs">arcs</a> or 
            <a href="#camera_circles">circles</a>, with <a href="#camera_mouse">user mouse control</a>,
            or even get <a href="#camera_dolly">dolly zooms</a> by changing the camera focal length.
            Note that we never train our model on posed data — this camera control capability 
            simply falls out of training a model to be conditioned on tracks.
          </p>
        </div>
      </div>
    </div>

    <!-- Dolly Zoom Videos -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h3 class="title is-4" id="camera_dolly">Dolly Zooms</h3>
      </div>
    </div>
    <div class="columns is-multiline is-variable is-4">

      <div class="column is-full">
        <div class="has-text-centered">
          <video class="lazy-video" poster="" muted loop playsinline>
            <source data-src="./static/videos/main/camera_dolly.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>

    <!-- Arcing Videos -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h3 class="title is-4" id="camera_arcs">Arcing</h3>
      </div>
    </div>
    <div class="columns is-multiline is-variable is-4">

      <div class="column is-full">
        <div class="has-text-centered">
          <video class="lazy-video" poster="" muted loop playsinline>
            <source data-src="./static/videos/main/camera_arcs.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>

    <!-- Circle Videos -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h3 class="title is-4" id="camera_circles">Circles</h3>
      </div>
    </div>
    <div class="columns is-multiline is-variable is-4">

      <div class="column is-full">
        <div class="has-text-centered">
          <video class="lazy-video" poster="" muted loop playsinline>
            <source data-src="./static/videos/main/camera_circles.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>

    <!-- Mouse Depth Videos -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h3 class="title is-4" id="camera_mouse">Mouse Controlled</h3>
      </div>
    </div>
    <div class="columns is-multiline is-variable is-4">

      <div class="column is-full">
        <div class="has-text-centered">
          <video class="lazy-video" poster="" muted loop playsinline>
            <source data-src="./static/videos/main/camera_mouse.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>

    <!-- Button to Gallery -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <a href="gallery_depth_dolly.html" class="see-more-button button is-secondary" target="_blank">
          See more examples
        </a>
      </div>
    </div>

  </div>
</section>


<!-- Primitives -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Title -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="primitives">Object Control with Geometric Primitives</h2>
      </div>
    </div>
    
    <!-- Description -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            We can also reinterpret mouse motions as manipulating a geometric primitive, such as a sphere. 
            By placing these tracks over an object that can be roughly approximated by the primitive, 
            we can get a motion prompt with more fine-grained control over the object than with sparse mouse tracks alone.
            Again, <u style="color: red;"><i><b style="color: red;">the 
            results below are neither real-time nor causal</b></i></u> (<a href="#caveat">see above</a>).
          </p>

          <p style="margin-top: 1rem;">
            In the bottom row we show a funny example of what might happen 
            if you don't use a spherical primitive.
          </p>
        </div>
      </div>
    </div>

    <!-- Videos -->
    <div class="columns is-multiline is-variable is-4">

      <div class="column is-full">
        <div class="has-text-centered">
          <video class="lazy-video" poster="" muted loop playsinline>
            <source data-src="./static/videos/main/primitives.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>

  </div>
</section>


<!-- Compositions -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Title -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="compositions">Object and Camera Control by Composition</h2>
      </div>
    </div>
    
    <!-- Description -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            We can also create motion prompts that result in simultaneous object and camera motion.
            To do this, we compose a camera control motion prompt with an object control motion prompt
            by adding the two tracks together. Technically, this is an approximation but is good enough 
            for camera trajectories that are not too extreme. Below we show examples of back and forth camera
            motion composed with head turns.
          </p>
        </div>
      </div>
    </div>

    <!-- Videos -->
    <div class="columns is-multiline is-variable is-4">

      <div class="column is-full">
        <div class="has-text-centered">
          <video class="lazy-video" poster="" muted loop playsinline>
            <source data-src="./static/videos/main/composition.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>

  </div>
</section>


<!-- Motion Transfer -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Title -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="transfer">Motion Transfer</h2>
      </div>
    </div>
    
    <!-- Description -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            Some motions are hard to create. In these cases, we can transfer a desired motion
            from a source video to a first frame. For example, below we puppeteer a <i>monkey</i>
            and a <i>skull</i> with a <i>moving face</i> and transfer the 
            <i>spinning of the Earth</i> to a <i>cat</i> and a <i>dog</i>.
          </p>
        </div>
      </div>
    </div>

    <!-- Videos -->
    <div class="columns is-multiline is-variable is-4">

      <div class="column is-full">
        <div class="has-text-centered">
          <video class="lazy-video" poster="" muted loop playsinline>
            <source data-src="./static/videos/main/transfer.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>

    <!-- Description -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p style="margin-top: 1rem;">
            Surprisingly, we find that our model can transfer motion even when applying extremely out of domain
            motions to images. For example, we can take the motion of a <i>monkey chewing on a banana</i> and apply it 
            to a <i>bird's eye photo of trees</i>, and a <i>brick wall</i>. In order to do this, we find that we need
            quite dense tracks — we use 1500, but only visualize a subset below so that the source video is still visible.
          </p>
          <p>
            The resulting videos exhibit an interesting effect in which pausing the video on any frame 
            removes the percept of the source video. That is, the <i>monkey</i> can only be 
            perceived when the video is playing, where a Gestalt common-fate effect occurs. This is related to 
            <a href="https://www.youtube.com/watch?v=1F5ICP9SYLU" target="_blank"> classic work</a>
            on motion perception
            <a id="ref13back" href="#ref13">[13]</a>, and a similar effect
            can be seen <a href="https://www.reddit.com/r/StableDiffusion/comments/17b4dfc/my_first_try_with_video/" target="_blank">here</a>.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-multiline is-variable is-4">

      <div class="column is-full">
        <div class="has-text-centered">
          <video class="lazy-video" poster="" muted loop playsinline>
            <source data-src="./static/videos/main/transfer_ood.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>

    <!-- Button to Gallery -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <a href="gallery_transfer_monkey.html" class="see-more-button button is-secondary" target="_blank">
          See more examples
        </a>
      </div>
    </div>

  </div>
</section>


<!-- Drag Based Image Editing -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Title -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="drag_edit">Drag Based Image Editing</h2>
      </div>
    </div>
    
    <!-- Description -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            There is a line of work that enables drag-based editing of images
            <a id="ref14back" href="#ref14">[14]</a>
            <a id="ref15back" href="#ref15">[15]</a>
            <a id="ref16back" href="#ref16">[16]</a>
            <a id="ref17back" href="#ref17">[17]</a>
            <a id="ref18back" href="#ref18">[18]</a>
            <a id="ref25back" href="#ref25">[25]</a>.
            We can also achieve a similar effect by conditioning our model
            on these drags. The set up is identical to the <a href="#interacting">"Interacting with an Image"</a>
            examples above. Similar to the insight from
            <a id="ref18back" href="#ref18">[18]</a>
            <a id="ref25back" href="#ref25">[25]</a>, the benefit 
            of using a video model is that we inherit a powerful video prior. Prior
            works also propose "edit-masks," which we can achieve by adding static 
            tracks to the conditioning, as shown on the far right.
          </p>
        </div>
      </div>
    </div>

    <!-- Videos -->
    <div class="columns is-multiline is-variable is-4">
      <div class="column is-full">
        <div class="has-text-centered">
          <video class="lazy-video" poster="" muted loop playsinline>
            <source data-src="./static/videos/main/drag.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>

  </div>
</section>


<!-- Motion Magnification -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Title -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="motionmag">Motion Magnification</h2>
      </div>
    </div>
    
    <!-- Description -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            Another application our model is motion magnification
            <a id="ref22back" href="#ref22">[22]</a>
            <a id="ref23back" href="#ref23">[23]</a>
            <a id="ref24back" href="#ref24">[24]</a>. This task involves taking
            a video with subtle motions and generating a new video in which
            these motions have been magnified, so that they are easier to see.
            We do this by running a tracking algorithm on an input video, 
            smoothing and magnifying the resulting tracks, 
            and then feeding the first frame of 
            the input video and magnified tracks to our model.
            Below, we show examples in which subtle breathing motion
            has been magnified.
          </p>
        </div>
      </div>
    </div>

    <!-- Videos -->
    <div class="columns is-multiline is-variable is-4">

      <div class="column is-full">
        <div class="has-text-centered">
          <video class="lazy-video" poster="" muted loop playsinline>
            <source data-src="./static/videos/main/motionmag.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>

  </div>
</section>



<!-- Failures -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Title -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Failures</h2>
      </div>
    </div>
    
    <!-- Description -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            Sometimes, motion prompts will have surprising and undesired
            effects. This often happens when the constructed motion 
            prompt underspecifies the desired behavior. For example, with the 
            <b>snow monkey</b> we want to arc the camera so that it is
            above the scene. However, the model attributes part of that motion
            to the monkey itself, resulting in it looking face-down into the water.
            In other cases, we get failures when not constructing the motion prompts
            carefully enough. In the <b>cow</b> example we inadverdently pin the 
            horns of the cow to the background. And sometimes, there is just 
            inherent ambiguity in motion, as with the <b>van Gogh</b> example.
          </p>
          <p>
            Failures can also illuminate limitations of the underlying video model.
            In the <b>chess</b> example, we see the model spontaneously conjuring 
            a pawn from thin air. A clearly unphysical behavior. And in the 
            <b>lion</b> example we see that the model apparently 
            does not know how a lion should move. These examples suggest that 
            motion prompts might be used to probe the capabilities and limitations 
            of video models.
          </p>
        </div>
      </div>
    </div>

    <!-- Videos -->
    <div class="columns is-multiline is-variable is-4">

      <div class="column is-full">
        <div class="has-text-centered">
          <video class="lazy-video" poster="" muted loop playsinline>
            <source data-src="./static/videos/main/failures.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>

  </div>
</section>


<!-- Different Text Prompts -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Title -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Effect of Text Prompts</h2>
      </div>
    </div>
    
    <!-- Description -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            Throughout this webpage
            we use text prompts that describe the image but not 
            the desired motion, so as to factor out the influence of text 
            on the motion as much as possible. We also try to keep the 
            text prompts simple.

            Here we show the effect of gradually more complex text prompts. For the most part,
            empirically we find that the text prompt does not have a
            significant effect on the motion. Sometimes, prompting
            for a specific result has unintended consequences such as 
            the pine tree example on the far right.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-multiline is-variable is-4">

      <div class="column is-full">
        <div class="has-text-centered">
          <video class="lazy-video" poster="" muted loop playsinline>
            <source data-src="./static/videos/main/text_sweep.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>

  </div>
</section>


<!-- Sparsity Sweep -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Title -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Effect of Track Sparsity</h2>
      </div>
    </div>
    
    <!-- Description -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            Track density is a knob that we allow users to tune themselves.
            Here we show the effect of track density on video generation for 
            a motion prompt with the intended effect of arcing the camera above 
            the scene. As can be seen, for low density tracks the motion is
            underspecified. For higher density tracks this motion is more
            precisely determined, but at the expense of giving the model 
            constraints that are too strict. Often, there is a happy middle 
            ground in which we can achieve the desired motion while keeping 
            emergent dynamics from the underlying video prior.
          </p>
        </div>
      </div>
    </div>

    <!-- Videos -->
    <div class="columns is-multiline is-variable is-4">
      <div class="column is-full">
        <div class="has-text-centered">
          <video class="lazy-video" poster="" muted loop playsinline>
            <source data-src="./static/videos/main/sparsity.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>

  </div>
</section>


<!-- Comparisons -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Title -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Comparisons</h2>
      </div>
    </div>
    
    <!-- Description -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            Here we show comparisons between our method, Image Conductor, and DragAnything,
            with the input motion visualized as a moving red dot.
            These are exactly the videos that participants in our human study were shown.
          </p>
        </div>
      </div>
    </div>

    <!-- Videos -->
    <div class="columns is-multiline is-variable is-4">
      <div class="column is-full" style="padding: 0;">
        <div class="has-text-centered">
          <video class="lazy-video" poster="" muted loop playsinline>
            <source data-src="./static/videos/main/comparison.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>

  </div>
</section>


<!-- Uncurated Samples -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Title -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="uncurated">Uncurated Samples and Cherry Picking</h2>
      </div>
    </div>
    
    <!-- Description -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            All videos shown so far have been cherry picked from four samples. Below we 
            show the uncurated, randomly sampled videos. These are exactly the set of
            videos from which we cherrypick. Note however that there is also implicit bias in how 
            we chose the inputs. For example, we found that our video model performs
            relatively well on animals, and as such we ended up focusing more on those kinds of inputs.
          </p>
          <p style="font-size: 0.85rem; padding: 0 20px;" id="causal">
            Aside: The sand video on the far right is a good example of how our model is not causal.
            The sand in the upper left corner begins to move before the mouse
            cursor even gets there. It anticipates that the mouse will move the sand there 
            because the model gets the full motion while generating the entire video.
          </p>
        </div>
      </div>
    </div>

    <!-- Videos -->
    <div class="columns is-multiline is-variable is-4">

      <div class="column is-full flush">
        <div class="has-text-centered">
          <video class="lazy-video" poster="" muted loop playsinline>
            <source data-src="./static/videos/main/uncurated_0.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="column is-full flush">
        <div class="has-text-centered">
          <video class="lazy-video" poster="" muted loop playsinline>
            <source data-src="./static/videos/main/uncurated_1.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="column is-full flush">
        <div class="has-text-centered">
          <video class="lazy-video" poster="" muted loop playsinline>
            <source data-src="./static/videos/main/uncurated_2.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>

  </div>
</section>



<!-- Related Work -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Work</h2>
      </div>
    </div>
    
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            This project is built on prior work led by Adam Harley, who (re)introduced
            point tracking
            <a href="#ref19">[19]</a>
            <a href="#ref20">[20]</a>, and also work on tracking
            led by Carl Doersch
            <a href="#ref21">[21]</a>.
          </p>
          <p>
            There is also a very large body of work on motion conditioned video generation, a subset 
            of which we cite below
            <a href="#ref1">[1]</a>
            <a href="#ref2">[2]</a>
            <a href="#ref3">[3]</a>
            <a href="#ref4">[4]</a>
            <a href="#ref5">[5]</a>
            <a href="#ref6">[6]</a>
            <a href="#ref7">[7]</a>
            <a href="#ref8">[8]</a>
            <a href="#ref9">[9]</a>
            <a href="#ref10">[10]</a>
            <a href="#ref11">[11]</a>. 
            Very very recent work by Xiao et al., 
            <a href="https://xizaoqu.github.io/trajattn/" target="_blank">Trajectory Attention</a>,
            also propose a way to condition video generation models on trajectories, 
            and focuses on the resulting fine-grained camera control and video-editing capabilities.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- References -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Title -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">References</h2>
      </div>
    </div>
    
    <!-- Description -->
    <div class="columns is-centered" style="font-size: 0.8rem;">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p id="ref1" style="margin-bottom: 0;">
            [1] Wang et al., "<a href="https://wzhouxiff.github.io/projects/MotionCtrl/" target="_blank">
              MotionCtrl: A Unified and Flexible Motion Controller for Video Generation</a>", SIGGRAPH 2024.
            <a href="#ref1back">↩</a>
          </p>
          <p id="ref2" style="margin-bottom: 0;">
            [2] Yin et al., "<a href="https://www.microsoft.com/en-us/research/project/dragnuwa/" target="_blank">
              DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory</a>", arXiv, 2023.
            <a href="#ref2back">↩</a>
          </p>
          <p id="ref3" style="margin-bottom: 0;">
            [3] Chen et al., "<a href="https://tsaishien-chen.github.io/MCDiff/" target="_blank">
              Motion-Conditioned Diffusion Model for Controllable Video Synthesis</a>", arXiv, 2023.
            <a href="#ref3back">↩</a>
          </p>
          <p id="ref4" style="margin-bottom: 0;">
            [4] Li et al., "<a href="https://liyaowei-stu.github.io/project/ImageConductor/" target="_blank">
              Image Conductor: Precision Control for Interactive Video Synthesis</a>", arXiv, 2024.
            <a href="#ref4back">↩</a>
          </p>
          <p id="ref5" style="margin-bottom: 0;">
            [5] Wu et al., "<a href="https://weijiawu.github.io/draganything_page/" target="_blank">
              DragAnything: Motion Control for Anything using Entity Representation</a>", ECCV 2024.
            <a href="#ref5back">↩</a>
          </p>
          <p id="ref6" style="margin-bottom: 0;">
            [6] Niu et al., "<a href="https://myniuuu.github.io/MOFA_Video/" target="_blank">
              MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model</a>", ECCV 2024.
            <a href="#ref6back">↩</a>
          </p>
          <p id="ref7" style="margin-bottom: 0;">
            [7] Wang et al., "<a href="https://videocomposer.github.io/" target="_blank">
              VideoComposer: Compositional Video Synthesis with Motion Controllability</a>", arXiv, 2023.
            <a href="#ref7back">↩</a>
          </p>
          <p id="ref8" style="margin-bottom: 0;">
            [8] Shi et al., "<a href="https://xiaoyushi97.github.io/Motion-I2V/" target="_blank">
              Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling</a>", SIGGRAPH 2024.
            <a href="#ref8back">↩</a>
          </p>
          <p id="ref9" style="margin-bottom: 0;">
            [9] Zhang et al., "<a href="https://ali-videoai.github.io/tora_video/" target="_blank">
              Tora: Trajectory-oriented Diffusion Transformer for Video Generation</a>", arXiv, 2024.
            <a href="#ref9back">↩</a>
          </p>
          <p id="ref10" style="margin-bottom: 0;">
            [10] Zhou et al., "<a href="https://zhtjtcz.github.io/TrackGo-Page/" target="_blank">
              TrackGo: A Flexible and Efficient Method for Controllable Video Generation</a>", arXiv, 2024.
            <a href="#ref10back">↩</a>
          </p>
          <p id="ref11" style="margin-bottom: 0;">
            [11] Lei et al., "<a href="https://yu-shaonian.github.io/Animate_Anything/" target="_blank">
              AnimateAnything: Consistent and Controllable Animation for video generation</a>", arXiv, 2024.
            <a href="#ref11back">↩</a>
          </p>
          <p id="ref12" style="margin-bottom: 0;">
            [12] Piccinelli et al., "<a href="https://github.com/lpiccinelli-eth/UniDepth" target="_blank">
              UniDepth: Universal Monocular Metric Depth Estimation</a>", CVPR 2024.
            <a href="#ref12back">↩</a>
          </p>
          <p id="ref13" style="margin-bottom: 0;">
            [13] Johansson et al., "<a href="https://link.springer.com/article/10.3758/BF03212378" target="_blank">
              Visual Perception of Biological Motion and a Model for its Analysis</a>", Perception & Psychophysics, 1973.
            <a href="#ref13back">↩</a>
          </p>
          <p id="ref14" style="margin-bottom: 0;">
            [14] Pan et al., "<a href="" target="_blank">
              Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold</a>", SIGGRAPH, 2023.
            <a href="#ref14back">↩</a>
          </p>
          <p id="ref15" style="margin-bottom: 0;">
            [15] Shi et al., "<a href="https://yujun-shi.github.io/projects/dragdiffusion.html" target="_blank">
              DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing</a>", CVPR 2024.
            <a href="#ref15back">↩</a>
          </p>
          <p id="ref16" style="margin-bottom: 0;">
            [16] Mou et al., "<a href="" target="_blank">
              DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models</a>", ICLR 2024.
            <a href="#ref16back">↩</a>
          </p>
          <p id="ref17" style="margin-bottom: 0;">
            [17] Geng et al., "<a href="https://dangeng.github.io/motion_guidance/" target="_blank">
              Motion Guidance: Diffusion-Based Image Editing with Differentiable Motion Estimators</a>", ICLR 2024.
            <a href="#ref17back">↩</a>
          </p>
          <p id="ref18" style="margin-bottom: 0;">
            [18] AlZayer et al., "<a href="https://magic-fixup.github.io/" target="_blank">
              Magic Fixup: Streamlining Photo Editing by Watching Dynamic Videos</a>", arXiv, 2024.
            <a href="#ref18back">↩</a>
          </p>
          <p id="ref19" style="margin-bottom: 0;">
            [19] Sand and Teller, "<a href="https://link.springer.com/article/10.1007/s11263-008-0136-6" target="_blank">
              Particle Video: Long-Range Motion Estimation Using Point Trajectories</a>", IJCV 2008.
            <a href="#ref19back">↩</a>
          </p>
          <p id="ref20" style="margin-bottom: 0;">
            [20] Harley et al., "<a href="https://particle-video-revisited.github.io/" target="_blank">
              Particle Video Revisited: Tracking Through Occlusions Using Point Trajectories</a>", ECCV 2022.
            <a href="#ref20back">↩</a>
          </p>
          <p id="ref21" style="margin-bottom: 0;">
            [21] Doersch et al., "<a href="https://tapvid.github.io/" target="_blank">
              TAP-Vid: A Benchmark for Tracking Any Point in a Video</a>", NeurIPS 2022.
            <a href="#ref21back">↩</a>
          </p>
          <p id="ref22" style="margin-bottom: 0;">
            [22] Liu et al., "<a href="https://people.csail.mit.edu/celiu/motionmag/motionmag.html" target="_blank">
              Motion Magnification</a>", SIGGRAPH 2005.
            <a href="#ref22back">↩</a>
          </p>
          <p id="ref23" style="margin-bottom: 0;">
            [23] Wu et al., "<a href="https://people.csail.mit.edu/mrub/evm/" target="_blank">
              Eulerian Video Magnification for Revealing Subtle Changes in the World</a>", SIGGRAPH 2012.
            <a href="#ref23back">↩</a>
          </p>
          <p id="ref24" style="margin-bottom: 0;">
            [24] Wadhwa et al., "<a href="https://people.csail.mit.edu/nwadhwa/phase-video/" target="_blank">
              Phase-based Video Motion Processing</a>", SIGGRAPH 2013.
            <a href="#ref24back">↩</a>
          </p>
          <p id="ref25" style="margin-bottom: 0;">
            [25] Rotstein et al., "<a href="https://arxiv.org/pdf/2411.16819" target="_blank">
              Pathways on the Image Manifold: Image Editing via Video Generation</a>", arXiv, 2024.
            <a href="#ref25back">↩</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section" id="bibtex">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{geng2024motionprompting,
  author    = {Geng, Daniel and Herrmann, Charles and Hur, Junhwa and Cole, Forrester and Zhang, Serena and Pfaff, Tobias and Lopez-Guevara, Tatiana and Doersch, Carl and Aytar, Yusuf and Rubinstein, Michael and Sun, Chen and Wang, Oliver and Owens, Andrew and Sun, Deqing},
  title     = {Motion Prompting: Controlling Video Generation with Motion Trajectories},
  journal   = {arXiv preprint arXiv:2412.02700},
  year      = {2024},
}</code></pre>
  </div>
</section>




<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
